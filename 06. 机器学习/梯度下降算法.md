# 梯度下降算法

梯度下降优化算法的主要思想是：通过测量参数向量代价函数的局部梯度，并不断沿着**降低梯度的方向**调整（迭代），直到梯度降为0，使代价函数降到最小值。

### 代价函数

又称损失函数，是用于找到**最优解**的目的函数。代价函数可以是任何能够衡量**预测值**（*prediction*）和真实值之间差异的函数。

以线性回归问题为例，线性模型就是对输入特征进行**加权求和**。记训练样本为
$$（x1,x2,x3...xn,y）$$
模型为
$$y'=\theta_0+\theta_1*x_1+\theta_2*x_2+...\theta_n*x_n
\\=h_{\theta}(x)
\\=\vec\theta * \vec x 
\\=\vec\theta^T \vec x
$$

* $y'$ 是预测值
* $x_i$ 是特征
* $\theta_i$ 是模型参数，包括特征权重和偏差项（截距项）。
* $\vec\theta$ 是模型的参数向量
* *X* 是数据实例的特征向量，包含 $x_0$ 到 $x_n$ ，且 $x_0$ 始终为1。
* $\vec\theta * \vec x$ 是两个向量的点积，即$\theta_0x_0+\theta_1x_1+\theta_2x_2+...\theta_nx_n$
* $\vec\theta^T \vec x$是两个向量（m×n和n×1）的矩阵乘积

对于线性回归模型，常用的代价函数是均方误差***MSE***(mean-square error).
$$J(\theta_0...\theta_n)=MSE={1\over{m}}\displaystyle\sum_{i=1}^m(y'_i-y_i)^2\\={1\over{m}}\displaystyle\sum_{i=1}^m(\theta^Tx_i-y_i)^2$$

最简单的**单变量**线性回归模型，即数据只有一个特征，数据是分散在平面上的点集，该模型将拟合出一条直线，$y'=\theta_0+\theta_1*x_1$，$\theta_0$是截距项， $\theta_1$是斜率。

那么代价函数是一个开口向上的二元一次函数，是一定可以求得最小值的。
::: tip 线性回归模型
* **假设**(Hypothesis):
$$h_{\theta}(x)=\theta_0+\theta_1*x_1+\theta_2*x_2+...\theta_n*x_n$$
* **参数**（Parameters）:
$$\theta_0...\theta_n$$
* **代价函数**(Cost Function):
$$MSE={1\over{m}}\displaystyle\sum_{i=1}^m(y'_i-y_i)^2\\={1\over{m}}\displaystyle\sum_{i=1}^m(\theta^Tx-y_i)^2$$
* **目标**（Goal）
$$J_{min}(\theta_0...\theta_n)$$
:::

### 梯度
代价函数对每个参数的**偏导数**就是**梯度**。
* 单变量函数中，梯度代表的是图像斜率的变化。
* 多变量函数中，梯度代表的是向量，变化最快的地方，即最陡峭的方向
我们为模型参数随机设置一个初始值，然后逐步改进，每一步都尝试降低一点成本函数，直到算法收敛出一个（局部）最小值。

$$repeat\ until\ convergence \{\\
\theta_j:=\theta_j-\alpha {{∂J(\vec \theta)}\over{∂\theta_j}}
\\
\}
$$

### 学习率
确定了方向后，我们还要控制前进的步长，就是学习率 $α$。同时还要设置循环终止条件（迭代次数或终止值）。

在梯度下降算法中，当我们接近局部最优解时，梯度下降算法会自动采用更小的幅度，这是因为当我们接近局部最优解时，导数会自动变得越来越小。